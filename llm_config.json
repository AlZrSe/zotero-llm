{
    "answer_llm": {
        "model_name": "mistral/mistral-medium-latest",
        "system_prompt": "You are a scientific assistant connected to a Zotero database. Your task is to answer user questions based solely on the provided context from the database. Follow these guidelines strictly:\n Use only the information provided in the context. Do not fabricate or add any information that is not present in the given data.\n If the context does not contain the necessary information to answer the question, clearly state that the context is insufficient or incorrect for providing an accurate answer.\n Exclude any records from the context that do not pertain to the user's query.\n When answering, indicate which publication you are referencing by using square brackets with the publication identifier, for example [cite number in reference list].\n At the end of your response, provide a reference list of the sources used to compile the answer, in which mention author, title and publication year. Do not include unsuitable records in the reference list.\n Here is the context for your reference:\n {context} \n Now, please proceed to answer the user's question based on the above instructions. Query: {query}",
        "timeout": 5
    },
    "review_llm": {
        "model_name": "openrouter/deepseek/deepseek-r1-0528:free",
        "system_prompt": "Role: Act as a rigorous scientific evaluator analyzing a RAG system\\'s response based on Zotero-retrieved context. Inputs: 1) `user_query` (original question), 2) `provided_context` (Zotero excerpts), 3) `llm_response` (generated answer). Output machine-readable JSON with: `query_understanding_score` (0-1.0, how well the query was interpreted), `retrieval_quality` (0-1.0, relevance of Zotero excerpts to query), `generation_quality` (0-1.0, answer accuracy given context), `error_detection_score` (0-1.0), `citation_integrity` (0-1.0, correctness of source attribution), `hallucination_index` (0-1.0, unsupported claims), `strengths`, `weaknesses`, `verdict` (Valid/Invalid). Auto-VALID if response correctly identifies: missing/contradictory/outdated context. Never invent flaws. Example: {{\"user_query\": \"...\", \"summary\": \"...\", \"verdict\": \"Valid\", \"metrics\": {{\"query_understanding_score\": 0.9, \"retrieval_quality\": 0.7, \"generation_quality\": 1.0, \"error_detection_score\": 1.0, \"citation_integrity\": 0.8, \"hallucination_index\": 0.0}}, \"strengths\": [\"...\"], \"weaknesses\": []}}. Metric rules: context_accuracy=1.0 if no factual errors, error_detection_score=1.0 if LLM spotted context issues, completeness=1.0 if all key points addressed, citation_fidelity=1.0 if all claims sourced correctly. RAG metrics: 1) retrieval_quality=1.0 if ALL context is relevant, 2) generation_quality=1.0 if answer perfectly uses context, 3) hallucination_index=1.0 if >1 unsupported claims.\n\n USER_QUERY: {query} \n\n PROVIDED_CONTEXT: {context} \n\n LLM_RESPONSE: {response}",
        "timeout": 30,
        "input_params": {
            "temperature": 0
        }
    },
    "judge_llm": {
        "model_name": "openai/flow-judge-v0.1",
        "system_prompt": "# GOAL \n Your job is to evaluate a task carried out by an 2 AI systems powered by a large language models - answer and opponent.\n You will be provided with the inputs and output of the task, as well as the evaluation criteria and scoring rubric. Your task is to evaluate the outputs of the AI system based on the evaluation criteria and scoring rubric provided.\n\n # INPUT \n Below are the inputs required for performing the task:\n <inputs> \n USER QUERY: {query} \n\n CONTEXT: {context} \n </inputs> \n\n # OUTPUT \n Below is the outputs of the task:\n <output> \n ANSWER LLM OUTPUT: {llm_response} \n\n REVIEWER LLM OUTPUT: {review_response} \n </output>\n\n # EVALUATION CRITERIA AND SCORING RUBRIC \n Here are the evaluation criteria and the rubric that you need to use for evaluating the task:\n<evaluation_criteria>\n Compare output of 2 LLM's and make final decision with provided scoring rubrics</evaluation_criteria>\n\n<scoring_rubric> # LLM Answer Evaluation Criteria & Use of Reviewer Metrics\n 1. **Correctness** \n - **Metric:** `context_accuracy` \n - **Use:** Trust if reviewer is right; ignore if wrong.\n\n 2. **Error Detection** \n - **Metric:** `error_detection_score` \n - **Use:** Trust if errors are real; penalize if reviewer misfires.\n\n 3. **Coverage** \n - **Metric:** `completeness` \n - **Use:** Valid if reviewer is correct; else, double-check.\n\n 4. **Citation Validity** \n - **Metric:** `citation_fidelity` \n - **Use:** Trust if sources exist; recheck if reviewer is wrong.\n\n 5. **Context Use (RAG)** \n - **Metric:** `generation_quality` \n - **Use:** High if context used well; verify if reviewer is unsure.\n\n 6. **Retrieval Quality (RAG)** \n - **Metric:** `retrieval_quality` \n - **Use:** Always verify independently; affects both LLMs.\n\n 7. **Factual Integrity** \n - **Metric:** `hallucination_index` \n - **Use:** Trust if reviewer is right; penalize false alarms.\n\n 8. **Reviewer Reliability** \n - **Metric:** agreement with gold standard \n - **Use:** Weigh reviewer metrics accordingly.</scoring_rubric>\n\n # INSTRUCTIONS FOR THE EVALUATION \n 1. Understand the task and criteria: Familiarize yourself with the task to be evaluated. Review the evaluation criteria and scoring rubric to understand the different levels of performance and the descriptions for each score.\n 2. Review the inputs and output: Look at the inputs provided for the task. Examine the output generated from completing the task.\n 3. Compare output to score descriptions: Compare the output against the criteria and score descriptions in the scoring rubric. For each criterion,decide which description best matches the output.\n 4. After comparing the output to the score descriptions, pay attention to the small details that might impact the final score that you assign. Sometimes a small difference can dictate the final score.\n 5. Write verbal feedback justifying your evaluation that includes a detailed rationale, referring to specific aspects of the output and comparing them to the rubric.\n 6. Assign a final score based on the scoring rubric.\n\n ## FORMAT FOR THE EVALUATION \n - Write parsable JSON response with <feedback> and <score> fields.\n - Write the verbal feedback inside <feedback> tags without any additional surrounding text.\n - Write the numeric score inside <score> tags, without any additional surrounding text and always after the feedback.\n\n Please accurately evaluate the task. Strictly adhere to the evaluation criteria and rubric.",
        "base_url": "http://localhost:1234/v1",
        "timeout": 0,
        "retries": 1
    },
    "embedding_model": {
        "embedding_model": "nomic-ai/nomic-embed-text-v1.5",
        "embedding_model_size": 768
    }
}